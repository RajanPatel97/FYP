# ArtGan-utils

First we need to install torch.

Follow http://torch.ch/docs/getting-started.html

You may need to install cmake and set path via

```
export PATH=CMAKE_PATH/bin:$PATH
```

Things done to install Torch:

Solve incompatibility CUDA9.0 and Torch (reason half-precision operators) 

```
export TORCH_NVCC_FLAGS="-D__CUDA_NO_HALF_OPERATORS__"
```

And maybe also use gcc6

```
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 10
```

```
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 10
```

generate_cnn_features.py: generate the image features from a pretrained net.
data_format_embeddings.py: here I am trying to set up the files in the same format as in the CVPR16 code.
tensors_to_t7.lua: Transforms the numpy image features to t7, which is the format needed by torch.



Instructions:

It is a little bit convoluted right now. Check the variables pointing to the data in data_format_embedding and generate_cnn_features. 

Running data_format_embeddings.py + text_to_t7.lua will generate the necessary data structure + text files.

Then running generate_cnn_features.py + tensors_to_t7.lua will generate the image features. You need to copy the folder ./images with all the features to the folder generated by data_format_embedding.py.

Then you run using the code https://github.com/reedscot/cvpr2016 this:

th train_sje_hybrid.lua   -data_dir eg/art_data   -image_dir images   -ids_file trainvalids.txt   -learning_rate 0.0007   -symmetric 1   -max_epochs 200   -savefile sje_art_c10_hybrid   -num_caption 10   -gpuid 1   -print_every 10 -image_dim 2048 -doc_length 200

Change data_dir to point where all the generated data is.

###<a href="http://arxiv.org/abs/1605.05395">Learning Deep Representations of Fine-grained Visual Descriptions</a>
Scott Reed, Zeynep Akata, Honglak Lee, Bernt Schiele

<img src="images/description_embedding.jpg" width="500px" height="250px"/>

#####How to train a char-CNN-RNN model:

1. Download the [birds](https://drive.google.com/open?id=0B0ywwgffWnLLZW9uVHNjb2JmNlE)
 and [flowers](https://drive.google.com/open?id=0B0ywwgffWnLLcms2WWJQRFNSWXM) data.
2. Modify the training script (e.g. `train_cub_hybrid.sh` for birds) to point to your data directory.
3. Run the training script: `./train_cub_hybrid.sh`

#####How to evaluate:

1. Train a model (see above).
2. Modify the eval bash script (e.g. `eval_cub_cls.sh` for birds) to point to your saved checkpoint.
3. Run the eval script: `./eval_cub_cls.sh`

#####Pretrained models:

* [Char-CNN-RNN for birds](https://drive.google.com/open?id=0B0ywwgffWnLLYUNVWVV5Sm1xcWc)
* [Char-CNN-RNN for flowers](https://drive.google.com/open?id=0B0ywwgffWnLLV205RXF4Y2hFY1E)

#####Citation

If you find this work useful, please cite as follows:

```
@inproceedings{reed2016learning, 	
 title = {Learning Deep Representations of Fine-Grained Visual Descriptions,
 booktitle = {IEEE Computer Vision and Pattern Recognition},
 year = {2016},
 author = {Scott Reed and Zeynep Akata and Bernt Schiele and Honglak Lee},
}
```

